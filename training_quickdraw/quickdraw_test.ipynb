{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/babi/Documents/GitHub/Quickdraw1/training_quickdraw\n",
      "2.4.0\n",
      "['angel'] 7729\n",
      "Train-Set:  Samples 7200\n",
      "Test-Set:  Samples 1800\n",
      "['alarm clock'] 1025\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import image\n",
    "import random as rd\n",
    "import math as m\n",
    "from PIL import Image\n",
    "\n",
    "import requests as req\n",
    "import npzviewer\n",
    "import wget\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "#from google.cloud import storage\n",
    "#client = storage.Client()\n",
    "import copy\n",
    "#pip install --user google-cloud-storage\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "##########\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def download_data(categories,number_of_samples):\n",
    "\n",
    "    number_of_categories = len(categories)\n",
    "    data = np.array([], dtype=np.int64).reshape(0,784)\n",
    "    for i in range(number_of_categories):\n",
    "        filename = cats[i][0] + '.npy'\n",
    "        filename = filename.replace(\" \",\"%20\")\n",
    "        \n",
    "        url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'+filename\n",
    "        \n",
    "        r = req.get(url, allow_redirects=True)\n",
    "        #filename = os.path.basename(url)\n",
    "        #filename = filename.replace(\"%20\",\"\")\n",
    "        open(filename, 'wb').write(r.content)\n",
    "        \n",
    "        data = np.vstack([data,load_data(filename,number_of_samples)])\n",
    "        \n",
    "        os.remove(filename)\n",
    "        print(i+1,'/',number_of_categories,' ',filename.replace(\"%20\",\"\"))\n",
    "    return data\n",
    "\n",
    "def load_data(name,n):\n",
    "    filename = name\n",
    "    label = name\n",
    "    data = np.load(filename)\n",
    "    #data = np.ndarray.reshape(data,len(data),28,28)\n",
    "    return data[0:n,:]\n",
    "\n",
    "#randomize data and labels\n",
    "def randomize_data(data,labels,seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(data)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(cat_id)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#load categories\n",
    "cats = open(\"categories.txt\",'r')\n",
    "reader = csv.reader(cats)\n",
    "cats = [row for row in reader]\n",
    "cats = cats[1:10]\n",
    "\n",
    "\n",
    "number_of_categories = len(cats)\n",
    "number_of_samples = 1000\n",
    "\n",
    "cat_id = np.repeat(range(number_of_categories),number_of_samples)+1\n",
    "\n",
    "#load data from google storage\n",
    "#d = download_data(cats[0:number_of_categories],number_of_samples)\n",
    "#np.savetxt('data_10_1000.csv', d, delimiter=',')\n",
    "\n",
    "#load data from file\n",
    "d = np.loadtxt('data_9_1000.csv', delimiter=',')\n",
    "data = np.reshape(d,(len(d),28,28))\n",
    "\n",
    "#reshape data into 28x28\n",
    "#data = d.copy()\n",
    "#data = np.reshape(data,(len(d),28,28))\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#randomize data and cat_id\n",
    "randomize_data(data,cat_id,67)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#TEST Show\n",
    "i=rd.randint(1,number_of_categories*number_of_samples)\n",
    "plt.imshow(data[i])\n",
    "print(cats[cat_id[i]-1],i)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#split data\n",
    "training = 0.8\n",
    "test = 1.-training\n",
    "\n",
    "x_train = data[0:m.floor(training*len(data))]/ 255.0\n",
    "y_train = cat_id[0:m.floor(training*len(cat_id))]\n",
    "\n",
    "x_test = data[m.ceil(training*len(data)):len(data)]/ 255.0\n",
    "y_test = cat_id[m.ceil(training*len(cat_id)):len(cat_id)]\n",
    "\n",
    "print('Train-Set: ','Samples',np.shape(x_train)[0])\n",
    "print('Test-Set: ','Samples',np.shape(x_test)[0])\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#TEST Show\n",
    "i=rd.randint(1,number_of_categories*number_of_samples)\n",
    "plt.imshow(x_train[i])\n",
    "print(cats[cat_id[i]-1],i)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(number_of_categories),\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Compile and Fit in Batches\n",
    "x_train_len = len(x_train)\n",
    "y_train_len = len(y_train)\n",
    "#x_test_len = len(x_test)\n",
    "#y_test_len = len(y_test)\n",
    "\n",
    "#batches = int(number_of_categories/5)\n",
    "#end = -1\n",
    "#for i in range(batches):\n",
    "#    \n",
    "#    start = int(end+1)\n",
    "#    end = int((i+1)*1/batches*x_train_len)\n",
    "#    \n",
    "#    x_train = x_train[start:end,:,:]\n",
    "#    y_train = y_train[start:end]\n",
    "#    #x_test = x_test[start:end,:]\n",
    "#    #y_test = y_test[start:end]\n",
    "#\n",
    "#    # Run with new data\n",
    "#    model.compile(optimizer='adam',\n",
    "#                  loss=loss_fn,\n",
    "#                  metrics=['accuracy'])\n",
    "#    \n",
    "#    model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
