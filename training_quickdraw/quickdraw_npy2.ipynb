{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import image\n",
    "\n",
    "import math as m\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import requests as req\n",
    "import csv\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import copy\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_npy(categories,number_of_samples):\n",
    "\n",
    "    number_of_categories = len(categories)\n",
    "    data = np.array([], dtype=np.int64).reshape(0,784)\n",
    "    for i in range(number_of_categories):\n",
    "        filename = categories[i][0] + '.npy'\n",
    "        filename = filename.replace(\" \",\"%20\")\n",
    "        \n",
    "        url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'+filename\n",
    "        \n",
    "        r = req.get(url, allow_redirects=True)\n",
    "        #filename = os.path.basename(url)\n",
    "        #filename = filename.replace(\"%20\",\"\")\n",
    "        open(filename, 'wb').write(r.content)\n",
    "        \n",
    "        data = np.vstack([data,load_data(filename,number_of_samples)])\n",
    "        \n",
    "        os.remove(filename)\n",
    "        print(i+1,'/',number_of_categories,' ',filename.replace(\"%20\",\"\"))\n",
    "    return data\n",
    "\n",
    "def load_data(name,n):\n",
    "    filename = name\n",
    "    label = name\n",
    "    data = np.load(filename)\n",
    "    #data = np.ndarray.reshape(data,len(data),28,28)\n",
    "    return data[0:n,:]\n",
    "\n",
    "#randomize data and labels\n",
    "def shuff(data,label):\n",
    "    \n",
    "    s = np.shape(data)\n",
    "    n = s[0]\n",
    "    \n",
    "    l = len(label)\n",
    "    label_new = np.zeros(l)\n",
    "    \n",
    "    if len(2*s)==2:\n",
    "        d = 1\n",
    "        m = 1\n",
    "        data_new = np.zeros((n,m))\n",
    "\n",
    "    elif len(2*s)==4:\n",
    "        d = 2\n",
    "        m = s[1]\n",
    "        data_new = np.zeros((n,m))\n",
    "    \n",
    "    orderid = random.sample(range(n), n)\n",
    "    \n",
    "    #l1 = list(range(n))\n",
    "    #l2 = list(range(n))\n",
    "    ##Kernel dies\n",
    "    #np.random.seed(seed) \n",
    "    #np.random.shuffle(l1)\n",
    "    #np.random.seed(seed)\n",
    "    #np.random.shuffle(l2)   \n",
    "    \n",
    "    for i in range(n):\n",
    "        data_new[i] = data[orderid[i]]\n",
    "        label_new[i] = int(label[orderid[i]])\n",
    "        \n",
    "    #del data, label\n",
    "    return data_new, label_new\n",
    "\n",
    "def download_and_save(number_of_categories,number_of_samples):\n",
    "\n",
    "    x = [random.randint(0, 345) for p in range(0, number_of_categories)]\n",
    "\n",
    "    #load categories\n",
    "    categories = open(\"categories.txt\",'r')\n",
    "    reader = csv.reader(categories)\n",
    "    categories = [row for row in reader]\n",
    "    categories = [categories[row] for row in x]\n",
    "    \n",
    "    cat_id = np.repeat(x,number_of_samples)\n",
    "    \n",
    "    d = download_npy(categories,number_of_samples)\n",
    "    \n",
    "    ##save Data\n",
    "    #filename = 'dataset/data_{}_{}.csv'.format(number_of_categories,number_of_samples)\n",
    "    #np.savetxt(filename, d, delimiter=',')\n",
    "    \n",
    "    #save Categories\n",
    "    filename = 'dataset/cat_{}_{}.csv'.format(number_of_categories,number_of_samples)\n",
    "    with open(filename, 'w') as f: \n",
    "        write = csv.writer(f) \n",
    "        write.writerows(categories)\n",
    "    return d, categories, cat_id\n",
    "    \n",
    "def data_from_file(number_of_categories,number_of_samples):\n",
    "    \n",
    "    categories_all = open(\"categories.txt\",'r')\n",
    "    reader = csv.reader(categories_all)\n",
    "    categories_all = [row for row in reader]\n",
    "    \n",
    "    filename = 'dataset/data_{}_{}.csv'.format(number_of_categories,number_of_samples)\n",
    "    d = np.loadtxt(filename, delimiter=',')\n",
    "    \n",
    "    filename = 'dataset/cat_{}_{}.csv'.format(number_of_categories,number_of_samples)\n",
    "    categories = open(filename,'r')\n",
    "    reader = csv.reader(categories)\n",
    "    categories = [row for row in reader]\n",
    "    \n",
    "    cat_id = np.repeat([categories_all.index(x) for x in categories],number_of_samples)\n",
    "    return d, categories, cat_id\n",
    "\n",
    "def download_specific_data(category_ids, number_of_samples):\n",
    "    \n",
    "    number_of_categories = len(category_ids)\n",
    "    \n",
    "    #load categories\n",
    "    categories = open(\"categories.txt\",'r')\n",
    "    reader = csv.reader(categories)\n",
    "    categories = [row for row in reader]\n",
    "    categories = [categories[row] for row in category_ids]\n",
    "    \n",
    "    cat_id = np.repeat(category_ids,number_of_samples)\n",
    "    \n",
    "    d = download_npy(categories,number_of_samples)\n",
    "    \n",
    "    #save Data\n",
    "    filename = 'dataset/data_{}_{}.csv'.format(number_of_categories,number_of_samples)\n",
    "    np.savetxt(filename, d, delimiter=',')\n",
    "    \n",
    "    #save Categories\n",
    "    filename = 'dataset/cat_{}_{}.csv'.format(number_of_categories,number_of_samples)\n",
    "    with open(filename, 'w') as f: \n",
    "        write = csv.writer(f) \n",
    "        write.writerows(categories)\n",
    "    return d, categories, cat_id\n",
    "    \n",
    "\n",
    "def predict_image(path_image):\n",
    "    image = Image.open(path_image)\n",
    "\n",
    "    image_resize = image.resize((28,28))\n",
    "    image_small= asarray(image_resize)\n",
    "    image_small = abs(image_small-255.)/255.\n",
    "    image_small = image_small[:, :, 0]\n",
    "    image_small = np.expand_dims(image_small, axis=0)\n",
    "    \n",
    "    predictions = model.predict(image_small)\n",
    "    print(cats[np.argmax(predictions)])\n",
    "    print(image_small.shape)\n",
    "    plt.imshow(image_resize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load categories\n",
    "categories = open(\"categories.txt\",'r')\n",
    "reader = csv.reader(categories)\n",
    "categories = [row for row in reader]\n",
    "#categories = [categories[row] for row in category_ids]\n",
    "\n",
    "filename = 'dataset/cat_{}_{}.csv'.format(345,1000)\n",
    "with open(filename, 'w') as f: \n",
    "    write = csv.writer(f) \n",
    "    write.writerows(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ncat = 5\n",
    "nsam = 1000\n",
    "\n",
    "##load data\n",
    "#d, cats = download_and_save(ncat,nsam)\n",
    "d, cats, cat_id = data_from_file(ncat,nsam)\n",
    "\n",
    "categories_all = open('categories.txt','r')\n",
    "reader = csv.reader(categories_all)\n",
    "categories_all = [row for row in reader]\n",
    "\n",
    "#cat_id = np.repeat(range(ncat),nsam)\n",
    "\n",
    "#print('Categories: ', cats)\n",
    "\n",
    "print(np.shape(d),' ',np.shape(cat_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle data\n",
    "data,cat_id = shuff(d,cat_id)\n",
    "\n",
    "#reshape data into 28x28\n",
    "data = np.reshape(data,(len(data),28,28))\n",
    "print(np.shape(data),' ',np.shape(cat_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "training = 0.8\n",
    "test = 1.-training\n",
    "\n",
    "x_train = data[0:m.floor(training*len(data))]/ 255.0\n",
    "x_test = data[m.ceil(training*len(data)):len(data)]/ 255.0\n",
    "\n",
    "y_train = cat_id[0:m.floor(training*len(cat_id))]\n",
    "y_test = cat_id[m.ceil(training*len(cat_id)):len(cat_id)]\n",
    "\n",
    "\n",
    "print('Train-Set: ','Samples',np.shape(x_train)[0],'/ Labels', np.shape(y_train)[0])\n",
    "print('Test-Set: ','Samples',np.shape(x_test)[0],'/ Labels', np.shape(y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST Show\n",
    "i=random.randint(1,len(x_train))\n",
    "plt.imshow(x_train[i])\n",
    "print(categories_all[int(cat_id[i])],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_ff():\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(512, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(256, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(345),\n",
    "      tf.keras.layers.Softmax()\n",
    "    ])\n",
    "    \n",
    "    #define lossfunction\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Compile: set optimizer, lossfunction, error metric\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "history = model.fit(x_train, y_train,batch_size = 256, epochs=5)\n",
    "\n",
    "#test the model\n",
    "print('Test')\n",
    "model.evaluate(x_test,  y_test, verbose=2);\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncat = 5\n",
    "nsam = 1000\n",
    "\n",
    "training = 0.8\n",
    "test = 1.-training\n",
    "\n",
    "categories_all_id = [random.randint(0, 345) for p in range(0, 345)]\n",
    "\n",
    "model = define_model_ff()\n",
    "\n",
    "for id in range(5,350,ncat):\n",
    "    \n",
    "    category_ids = categories_all_id[id-ncat:id]\n",
    "    print(id-ncat, id, [categories_all[i] for i in category_ids])\n",
    "    \n",
    "    d, cats, cat_id = download_specific_data(category_ids, nsam)\n",
    "    \n",
    "    #Shuffle data\n",
    "    data,cat_id = shuff(d,cat_id)\n",
    "    #reshape data into 28x28\n",
    "    data = np.reshape(data,(len(data),28,28))\n",
    "\n",
    "    x_train = data[0:m.floor(training*len(data))]/ 255.0\n",
    "    x_test = data[m.ceil(training*len(data)):len(data)]/ 255.0\n",
    "    y_train = cat_id[0:m.floor(training*len(cat_id))]\n",
    "    y_test = cat_id[m.ceil(training*len(cat_id)):len(cat_id)]\n",
    "    \n",
    "    \n",
    "    #train the model\n",
    "    model.fit(x_train, y_train,batch_size = 32, epochs=5)\n",
    "    \n",
    "    #test the model\n",
    "    model.evaluate(x_test,  y_test, verbose=2);\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(ncat, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to 4D\n",
    "data4 = np.reshape(data,(len(data),28,28,1))\n",
    "\n",
    "#split data\n",
    "training = 0.8\n",
    "test = 1.-training\n",
    "\n",
    "x_train = data4[0:m.floor(training*len(data4))]/ 255.0\n",
    "y_train = cat_id[0:m.floor(training*len(cat_id))]\n",
    "\n",
    "x_test = data4[m.ceil(training*len(data4)):len(data4),:]/ 255.0\n",
    "y_test = cat_id[m.ceil(training*len(cat_id)):len(cat_id)]\n",
    "\n",
    "print('Train-Set: ','Samples',np.shape(x_train)[0],'/ Labels', np.shape(y_train)[0])\n",
    "print('Test-Set: ','Samples',np.shape(x_test)[0],'/ Labels', np.shape(y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model and Train\n",
    "model = define_model()\n",
    "history = model.fit(x_train, y_train,batch_size = 256, epochs=10)\n",
    "\n",
    "_, acc = model.evaluate(x_test,  y_test, verbose=2);\n",
    "\n",
    "print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "# stores scores\n",
    "scores, histories = list(), list()\n",
    "scores.append(acc)\n",
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Test-Set\n",
    "i=random.randint(1,len(x_test))\n",
    "print(cats)\n",
    "print('Pic is ',cats[np.argmax(predictions[i])])\n",
    "plt.figure()\n",
    "plt.imshow(x_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Sketch\n",
    "predict_image('dataset/broccoli1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"qd_cnn_345_1000.h5\"\n",
    "saved_model_dir = 'save/'+str(name)\n",
    "model.save(saved_model_dir)\n",
    "print('Model Saved to '+saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(number_of_categories),\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "#define lossfunction\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# Compile and Fit in Batches\n",
    "x_train_len = len(x_train)\n",
    "y_train_len = len(y_train)\n",
    "#x_test_len = len(x_test)\n",
    "#y_test_len = len(y_test)\n",
    "\n",
    "batches = int(number_of_categories*number_of_samples/10)\n",
    "end = -1\n",
    "for i in range(batches):\n",
    "    \n",
    "    start = int(end+1)\n",
    "    end = int((i+1)*1/batches*x_train_len)\n",
    "    \n",
    "    x_train2 = x_train[start:end,:,:].copy()\n",
    "    y_train2 = y_train[start:end].copy()\n",
    "    #x_test = x_test[start:end,:]\n",
    "    #y_test = y_test[start:end]\n",
    "\n",
    "    # Run with new data\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if i%10:\n",
    "        print('Batch No. ',i,'/',batches)\n",
    "    model.fit(x_train2, y_train2, epochs=5, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
